{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "FKTWn4MIrWgG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (\n",
        "    classification_report, confusion_matrix, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, roc_curve, auc\n",
        ")\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------- User variables ----------------------\n",
        "CSV_PATH = \"german_credit_data.csv\"  # change if needed\n",
        "RANDOM_STATE = 42\n",
        "TEST_SIZE = 0.2\n",
        "RESULTS_DIR = \"results\"\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)"
      ],
      "metadata": {
        "id": "Z8Hiz6QiukXB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------- Load data ----------------------\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "print(\"Loaded data shape:\", df.shape)\n",
        "print(\"Columns:\", df.columns.tolist())\n",
        "print(\"\\nFirst 5 rows:\\n\", df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xij92vX2utKL",
        "outputId": "e10aa55d-a695-4692-8e0a-122f6ba706ea"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded data shape: (1000, 10)\n",
            "Columns: ['Unnamed: 0', 'Age', 'Sex', 'Job', 'Housing', 'Saving accounts', 'Checking account', 'Credit amount', 'Duration', 'Purpose']\n",
            "\n",
            "First 5 rows:\n",
            "    Unnamed: 0  Age     Sex  Job Housing Saving accounts Checking account  \\\n",
            "0           0   67    male    2     own             NaN           little   \n",
            "1           1   22  female    2     own          little         moderate   \n",
            "2           2   49    male    1     own          little              NaN   \n",
            "3           3   45    male    2    free          little           little   \n",
            "4           4   53    male    2    free          little           little   \n",
            "\n",
            "   Credit amount  Duration              Purpose  \n",
            "0           1169         6             radio/TV  \n",
            "1           5951        48             radio/TV  \n",
            "2           2096        12            education  \n",
            "3           7882        42  furniture/equipment  \n",
            "4           4870        24                  car  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------- Identify target ----------------------\n",
        "# Common Kaggle column name: 'Risk' with values like 'good'/'bad'\n",
        "# If your dataset uses a different target column, change TARGET_COL accordingly.\n",
        "TARGET_COL = None\n",
        "for candidate in [\"Risk\", \"risk\", \"Target\", \"target\", \"Class\", \"class\"]:\n",
        "    if candidate in df.columns:\n",
        "        TARGET_COL = candidate\n",
        "        break\n",
        "if TARGET_COL is None:\n",
        "    # fallback: try to infer last column as target\n",
        "    TARGET_COL = df.columns[-1]\n",
        "\n",
        "print(f\"\\nUsing target column: {TARGET_COL}\")\n",
        "\n",
        "# Map target to binary 0/1 if needed\n",
        "if df[TARGET_COL].dtype == object:\n",
        "    unique_vals = df[TARGET_COL].unique()\n",
        "    print(\"Target unique values:\", unique_vals)\n",
        "    # Map 'good'->0, 'bad'->1 if present\n",
        "    if set(map(str.lower, unique_vals)) >= {\"good\", \"bad\"}:\n",
        "        df[TARGET_COL] = df[TARGET_COL].map(lambda x: 1 if str(x).lower() == \"bad\" else 0)\n",
        "    else:\n",
        "        # generic mapping: most frequent -> 0, others -> 1\n",
        "        most_freq = df[TARGET_COL].value_counts().idxmax()\n",
        "        df[TARGET_COL] = (df[TARGET_COL] != most_freq).astype(int)\n",
        "else:\n",
        "    # If numeric already, ensure binary 0/1\n",
        "    unique_vals = sorted(df[TARGET_COL].unique())\n",
        "    if len(unique_vals) > 2:\n",
        "        print(\"Warning: Target appears to have >2 classes; this script is for binary classification.\")\n",
        "    # if values other than 0/1 exist, map min->0, max->1\n",
        "    if set(unique_vals) != {0,1} and len(unique_vals) == 2:\n",
        "        mapping = {unique_vals[0]: 0, unique_vals[1]: 1}\n",
        "        df[TARGET_COL] = df[TARGET_COL].map(mapping)\n",
        "\n",
        "print(\"\\nTarget value counts:\\n\", df[TARGET_COL].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_TkhMZau8fo",
        "outputId": "c1dc0fb2-6d1f-4d01-b0a8-ab0f1c4a29a2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Using target column: Purpose\n",
            "Target unique values: ['radio/TV' 'education' 'furniture/equipment' 'car' 'business'\n",
            " 'domestic appliances' 'repairs' 'vacation/others']\n",
            "\n",
            "Target value counts:\n",
            " Purpose\n",
            "1    663\n",
            "0    337\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------- Feature separation ----------------------\n",
        "# Remove target & ID-like columns if present\n",
        "possible_id_cols = [c for c in df.columns if c.lower() in (\"id\", \"index\")]\n",
        "X = df.drop(columns=[TARGET_COL] + possible_id_cols)\n",
        "y = df[TARGET_COL].copy()\n",
        "\n",
        "# Detect numeric vs categorical:\n",
        "numeric_cols = X.select_dtypes(include=[\"number\"]).columns.tolist()\n",
        "cat_cols = X.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
        "\n",
        "# Some integer-coded categorical features (small number of unique values) -> treat as categorical\n",
        "for col in numeric_cols[:]:  # iterate over a copy\n",
        "    if X[col].nunique() <= 10 and X[col].dtype in [np.int64, np.int32, np.int16, np.int8]:\n",
        "        cat_cols.append(col)\n",
        "        numeric_cols.remove(col)\n",
        "\n",
        "print(\"\\nNumeric columns:\", numeric_cols)\n",
        "print(\"Categorical columns:\", cat_cols)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2_tEZiavm-s",
        "outputId": "7e9c1de3-d731-4355-d21d-f1563b06b519"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Numeric columns: ['Unnamed: 0', 'Age', 'Credit amount', 'Duration']\n",
            "Categorical columns: ['Sex', 'Housing', 'Saving accounts', 'Checking account', 'Job']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------- Preprocessing pipelines ----------------------\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"MISSING\")),\n",
        "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    (\"num\", numeric_transformer, numeric_cols),\n",
        "    (\"cat\", categorical_transformer, cat_cols)\n",
        "], remainder=\"drop\")\n",
        "\n",
        "# ---------------------- Train/test split ----------------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
        ")\n",
        "print(f\"\\nTrain shape: {X_train.shape}, Test shape: {X_test.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CozVfrx2vwCB",
        "outputId": "184b96b1-6411-475e-9be4-5ac8aca12d35"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train shape: (800, 9), Test shape: (200, 9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------- Helper: evaluate model ----------------------\n",
        "def evaluate_model(name, model, X_test, y_test, show_plots=True):\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_prob = None\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        y_prob = model.predict_proba(X_test)[:, 1]\n",
        "    elif hasattr(model, \"decision_function\"):\n",
        "        y_prob = model.decision_function(X_test)\n",
        "    else:\n",
        "        y_prob = np.zeros(len(y_test))\n",
        "\n",
        "    pr = precision_score(y_test, y_pred, zero_division=0)\n",
        "    rc = recall_score(y_test, y_pred, zero_division=0)\n",
        "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
        "    roc_auc = roc_auc_score(y_test, y_prob)\n",
        "\n",
        "    print(f\"\\n=== {name} ===\")\n",
        "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1: {:.4f}, ROC-AUC: {:.4f}\".format(pr, rc, f1, roc_auc))\n",
        "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, zero_division=0))\n",
        "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "\n",
        "    if show_plots:\n",
        "        fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
        "        plt.figure(figsize=(6,5))\n",
        "        plt.plot(fpr, tpr, label=f\"{name} (AUC = {roc_auc:.3f})\")\n",
        "        plt.plot([0,1],[0,1], linestyle=\"--\")\n",
        "        plt.xlabel(\"False Positive Rate\")\n",
        "        plt.ylabel(\"True Positive Rate\")\n",
        "        plt.title(f\"ROC Curve â€” {name}\")\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(RESULTS_DIR, f\"roc_{name.replace(' ','_')}.png\"))\n",
        "        plt.close()\n",
        "\n",
        "    return {\"precision\": pr, \"recall\": rc, \"f1\": f1, \"roc_auc\": roc_auc}"
      ],
      "metadata": {
        "id": "eZh-L-qnv5Uy"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------- Models to train ----------------------\n",
        "models = {\n",
        "    \"LogisticRegression\": Pipeline(steps=[\n",
        "        (\"preprocessor\", preprocessor),\n",
        "        (\"clf\", LogisticRegression(max_iter=2000, random_state=RANDOM_STATE, solver=\"liblinear\"))\n",
        "    ]),\n",
        "    \"DecisionTree\": Pipeline(steps=[\n",
        "        (\"preprocessor\", preprocessor),\n",
        "        (\"clf\", DecisionTreeClassifier(random_state=RANDOM_STATE))\n",
        "    ]),\n",
        "    \"RandomForest\": Pipeline(steps=[\n",
        "        (\"preprocessor\", preprocessor),\n",
        "        (\"clf\", RandomForestClassifier(n_estimators=200, random_state=RANDOM_STATE, n_jobs=-1))\n",
        "    ])\n",
        "}\n",
        "\n",
        "trained_models = {}\n",
        "results = {}\n"
      ],
      "metadata": {
        "id": "lkaEWBM_xp5y"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------- Train and evaluate ----------------------\n",
        "for name, pipeline in models.items():\n",
        "    print(f\"\\nTraining: {name} ...\")\n",
        "    pipeline.fit(X_train, y_train)\n",
        "    trained_models[name] = pipeline\n",
        "    # Evaluate on test set\n",
        "    results[name] = evaluate_model(name, pipeline, X_test, y_test, show_plots=True)\n",
        "\n",
        "# Save evaluation summary to CSV\n",
        "pd.DataFrame(results).T.to_csv(os.path.join(RESULTS_DIR, \"model_results_summary.csv\"))\n",
        "print(\"\\nSaved results summary to:\", os.path.join(RESULTS_DIR, \"model_results_summary.csv\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrLLqQQ7xuMa",
        "outputId": "c13adfdc-d9bf-4ae9-a8e4-87d0b0b9cdcf"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training: LogisticRegression ...\n",
            "\n",
            "=== LogisticRegression ===\n",
            "Precision: 0.6910, Recall: 0.9248, F1: 0.7910, ROC-AUC: 0.5868\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.18      0.27        67\n",
            "           1       0.69      0.92      0.79       133\n",
            "\n",
            "    accuracy                           0.68       200\n",
            "   macro avg       0.62      0.55      0.53       200\n",
            "weighted avg       0.64      0.68      0.62       200\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 12  55]\n",
            " [ 10 123]]\n",
            "\n",
            "Training: DecisionTree ...\n",
            "\n",
            "=== DecisionTree ===\n",
            "Precision: 0.7111, Recall: 0.7218, F1: 0.7164, ROC-AUC: 0.5699\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.43      0.42      0.42        67\n",
            "           1       0.71      0.72      0.72       133\n",
            "\n",
            "    accuracy                           0.62       200\n",
            "   macro avg       0.57      0.57      0.57       200\n",
            "weighted avg       0.62      0.62      0.62       200\n",
            "\n",
            "Confusion Matrix:\n",
            " [[28 39]\n",
            " [37 96]]\n",
            "\n",
            "Training: RandomForest ...\n",
            "\n",
            "=== RandomForest ===\n",
            "Precision: 0.7176, Recall: 0.9173, F1: 0.8053, ROC-AUC: 0.5890\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.28      0.39        67\n",
            "           1       0.72      0.92      0.81       133\n",
            "\n",
            "    accuracy                           0.70       200\n",
            "   macro avg       0.68      0.60      0.60       200\n",
            "weighted avg       0.69      0.70      0.67       200\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 19  48]\n",
            " [ 11 122]]\n",
            "\n",
            "Saved results summary to: results/model_results_summary.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------- Train and evaluate ----------------------\n",
        "for name, pipeline in models.items():\n",
        "    print(f\"\\nTraining: {name} ...\")\n",
        "    pipeline.fit(X_train, y_train)\n",
        "    trained_models[name] = pipeline\n",
        "    # Evaluate on test set\n",
        "    results[name] = evaluate_model(name, pipeline, X_test, y_test, show_plots=True)\n",
        "\n",
        "# Save evaluation summary to CSV\n",
        "pd.DataFrame(results).T.to_csv(os.path.join(RESULTS_DIR, \"model_results_summary.csv\"))\n",
        "print(\"\\nSaved results summary to:\", os.path.join(RESULTS_DIR, \"model_results_summary.csv\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTkQ3WSSxyD8",
        "outputId": "c8cb55b7-81f5-431c-b84d-0e5cafcb344d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training: LogisticRegression ...\n",
            "\n",
            "=== LogisticRegression ===\n",
            "Precision: 0.6910, Recall: 0.9248, F1: 0.7910, ROC-AUC: 0.5868\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.18      0.27        67\n",
            "           1       0.69      0.92      0.79       133\n",
            "\n",
            "    accuracy                           0.68       200\n",
            "   macro avg       0.62      0.55      0.53       200\n",
            "weighted avg       0.64      0.68      0.62       200\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 12  55]\n",
            " [ 10 123]]\n",
            "\n",
            "Training: DecisionTree ...\n",
            "\n",
            "=== DecisionTree ===\n",
            "Precision: 0.7111, Recall: 0.7218, F1: 0.7164, ROC-AUC: 0.5699\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.43      0.42      0.42        67\n",
            "           1       0.71      0.72      0.72       133\n",
            "\n",
            "    accuracy                           0.62       200\n",
            "   macro avg       0.57      0.57      0.57       200\n",
            "weighted avg       0.62      0.62      0.62       200\n",
            "\n",
            "Confusion Matrix:\n",
            " [[28 39]\n",
            " [37 96]]\n",
            "\n",
            "Training: RandomForest ...\n",
            "\n",
            "=== RandomForest ===\n",
            "Precision: 0.7176, Recall: 0.9173, F1: 0.8053, ROC-AUC: 0.5890\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.28      0.39        67\n",
            "           1       0.72      0.92      0.81       133\n",
            "\n",
            "    accuracy                           0.70       200\n",
            "   macro avg       0.68      0.60      0.60       200\n",
            "weighted avg       0.69      0.70      0.67       200\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 19  48]\n",
            " [ 11 122]]\n",
            "\n",
            "Saved results summary to: results/model_results_summary.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------- Feature importance (Random Forest) ----------------------\n",
        "# We need feature names after preprocessing:\n",
        "rf_pipeline = trained_models.get(\"RandomForest\")\n",
        "if rf_pipeline is not None:\n",
        "    pre = rf_pipeline.named_steps[\"preprocessor\"]\n",
        "    # numeric feature names\n",
        "    num_features = numeric_cols\n",
        "    # categorical feature names after one-hot:\n",
        "    ohe = pre.named_transformers_[\"cat\"].named_steps[\"onehot\"]\n",
        "    # OneHotEncoder may not be present if no categorical columns; handle that\n",
        "    cat_feature_names = []\n",
        "    if cat_cols:\n",
        "        # build categories list\n",
        "        categories = ohe.categories_\n",
        "        for col, cats in zip(cat_cols, categories):\n",
        "            cat_feature_names += [f\"{col}__{str(c)}\" for c in cats]\n",
        "\n",
        "    feature_names = num_features + cat_feature_names\n",
        "\n",
        "    rf = rf_pipeline.named_steps[\"clf\"]\n",
        "    importances = rf.feature_importances_\n",
        "    fi = pd.Series(importances, index=feature_names).sort_values(ascending=False)\n",
        "    print(\"\\nTop 20 feature importances (Random Forest):\\n\", fi.head(20))\n",
        "    fi.head(50).to_csv(os.path.join(RESULTS_DIR, \"rf_feature_importances.csv\"))\n",
        "    print(\"Saved RF feature importances to:\", os.path.join(RESULTS_DIR, \"rf_feature_importances.csv\"))\n",
        "\n",
        "    # Plot top 15\n",
        "    plt.figure(figsize=(8,6))\n",
        "    fi.head(15).sort_values().plot(kind=\"barh\")\n",
        "    plt.title(\"Random Forest - Top 15 feature importances\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(RESULTS_DIR, \"rf_top15_importances.png\"))\n",
        "    plt.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ml2FtLzx6Fi",
        "outputId": "b2129f02-b2af-4216-e910-b48cfffe3425"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 20 feature importances (Random Forest):\n",
            " Credit amount                  0.213254\n",
            "Unnamed: 0                     0.186137\n",
            "Age                            0.164982\n",
            "Duration                       0.129676\n",
            "Checking account__little       0.024020\n",
            "Job__2                         0.022395\n",
            "Saving accounts__little        0.021771\n",
            "Checking account__MISSING      0.021134\n",
            "Housing__own                   0.021129\n",
            "Checking account__moderate     0.019999\n",
            "Sex__male                      0.019587\n",
            "Saving accounts__MISSING       0.019019\n",
            "Sex__female                    0.018103\n",
            "Saving accounts__moderate      0.017940\n",
            "Job__1                         0.017909\n",
            "Job__3                         0.017230\n",
            "Housing__free                  0.015189\n",
            "Housing__rent                  0.014746\n",
            "Saving accounts__quite rich    0.010976\n",
            "Checking account__rich         0.009663\n",
            "dtype: float64\n",
            "Saved RF feature importances to: results/rf_feature_importances.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------- Coefficients (Logistic Regression) ----------------------\n",
        "lr_pipeline = trained_models.get(\"LogisticRegression\")\n",
        "if lr_pipeline is not None:\n",
        "    # get feature names same as above using the preprocessor\n",
        "    pre = lr_pipeline.named_steps[\"preprocessor\"]\n",
        "    num_features = numeric_cols\n",
        "    cat_feature_names = []\n",
        "    if cat_cols:\n",
        "        ohe = pre.named_transformers_[\"cat\"].named_steps[\"onehot\"]\n",
        "        categories = ohe.categories_\n",
        "        for col, cats in zip(cat_cols, categories):\n",
        "            cat_feature_names += [f\"{col}__{str(c)}\" for c in cats]\n",
        "    feature_names = num_features + cat_feature_names\n",
        "\n",
        "    lr = lr_pipeline.named_steps[\"clf\"]\n",
        "    coef = lr.coef_.ravel()\n",
        "    coef_series = pd.Series(coef, index=feature_names).sort_values()\n",
        "    print(\"\\nTop positive coefficients (Logistic Regression):\\n\", coef_series.tail(10))\n",
        "    print(\"\\nTop negative coefficients (Logistic Regression):\\n\", coef_series.head(10))\n",
        "    coef_series.to_csv(os.path.join(RESULTS_DIR, \"lr_coefficients.csv\"))\n",
        "    print(\"Saved LR coefficients to:\", os.path.join(RESULTS_DIR, \"lr_coefficients.csv\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdagqWBjyCsu",
        "outputId": "1ca4e5d3-6d94-4696-e548-27d7f0ec33de"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top positive coefficients (Logistic Regression):\n",
            " Job__3                         0.094614\n",
            "Sex__female                    0.132497\n",
            "Checking account__moderate     0.209732\n",
            "Job__1                         0.216020\n",
            "Saving accounts__little        0.282796\n",
            "Job__2                         0.297910\n",
            "Duration                       0.357214\n",
            "Saving accounts__quite rich    0.364479\n",
            "Checking account__rich         0.386537\n",
            "Housing__own                   0.425232\n",
            "dtype: float64\n",
            "\n",
            "Top negative coefficients (Logistic Regression):\n",
            " Job__0                      -0.499224\n",
            "Credit amount               -0.433417\n",
            "Housing__free               -0.401975\n",
            "Checking account__little    -0.364665\n",
            "Saving accounts__rich       -0.220266\n",
            "Saving accounts__moderate   -0.176320\n",
            "Saving accounts__MISSING    -0.141369\n",
            "Checking account__MISSING   -0.122284\n",
            "Age                         -0.099309\n",
            "Unnamed: 0                  -0.077771\n",
            "dtype: float64\n",
            "Saved LR coefficients to: results/lr_coefficients.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------- Hyperparameter tuning example (Random Forest) ----------------------\n",
        "print(\"\\nStarting a quick GridSearchCV on RandomForest (small grid)...\")\n",
        "param_grid = {\n",
        "    \"clf__n_estimators\": [100, 200],\n",
        "    \"clf__max_depth\": [None, 6, 12],\n",
        "    \"clf__min_samples_split\": [2, 5]\n",
        "}\n",
        "grid = GridSearchCV(trained_models[\"RandomForest\"], param_grid, cv=5, scoring=\"roc_auc\", n_jobs=-1, verbose=0)\n",
        "grid.fit(X_train, y_train)\n",
        "print(\"Best params:\", grid.best_params_)\n",
        "print(\"Best CV ROC-AUC:\", grid.best_score_)\n",
        "best_rf = grid.best_estimator_\n",
        "# evaluate tuned model\n",
        "results[\"RandomForest_tuned\"] = evaluate_model(\"RandomForest_tuned\", best_rf, X_test, y_test, show_plots=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNtZTsOyyMWo",
        "outputId": "ed9e6776-2f28-4051-9d8c-7efafa87f3a8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting a quick GridSearchCV on RandomForest (small grid)...\n",
            "Best params: {'clf__max_depth': 12, 'clf__min_samples_split': 2, 'clf__n_estimators': 100}\n",
            "Best CV ROC-AUC: 0.6225716282320056\n",
            "\n",
            "=== RandomForest_tuned ===\n",
            "Precision: 0.7029, Recall: 0.9248, F1: 0.7987, ROC-AUC: 0.5864\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.22      0.33        67\n",
            "           1       0.70      0.92      0.80       133\n",
            "\n",
            "    accuracy                           0.69       200\n",
            "   macro avg       0.65      0.57      0.56       200\n",
            "weighted avg       0.67      0.69      0.64       200\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 15  52]\n",
            " [ 10 123]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------- Save best model to disk ----------------------\n",
        "# Choose final model by highest test ROC-AUC found above\n",
        "best_model_name = max(results.items(), key=lambda t: t[1][\"roc_auc\"])[0]\n",
        "print(\"\\nBest model by test ROC-AUC:\", best_model_name)\n",
        "best_model_obj = trained_models.get(best_model_name, None)\n",
        "if best_model_name == \"RandomForest_tuned\":\n",
        "    best_model_obj = best_rf\n",
        "\n",
        "if best_model_obj is not None:\n",
        "    joblib.dump(best_model_obj, os.path.join(RESULTS_DIR, f\"{best_model_name}.joblib\"))\n",
        "    print(\"Saved best model to:\", os.path.join(RESULTS_DIR, f\"{best_model_name}.joblib\"))\n",
        "\n",
        "print(\"\\nAll done. Check the results folder for saved plots, CSVs, and the model.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOEwi-gryfGR",
        "outputId": "9b690424-6668-45ae-9fc6-a86e590497fb"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Best model by test ROC-AUC: RandomForest\n",
            "Saved best model to: results/RandomForest.joblib\n",
            "\n",
            "All done. Check the results folder for saved plots, CSVs, and the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hk7L-_uOysn4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}